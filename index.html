<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dongzhi Jiang</title>

    <meta name="author" content="Dongzhi Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/bat.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-family:Comic Sans MS;font-size:32px">
                  Dongzhi Jiang
                </p>
                <p class="name" style="font-size:15px">I'm a PhD student in <a href="https://mmlab.ie.cuhk.edu.hk/">Multimedia Lab</a>, CUHK, supervised by Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/"> Hongsheng Li</a>. 
                <p class="name" style="font-size:15px">
                  Previously, I obtained my bachelor degree from the Computer Science Department, Harbin Institute of Technology, Shenzhen. I was supervised by Prof. <a href="https://scholar.google.com/citations?user=T8YbHBwAAAAJ&hl=zh-CN&oi=ao">Jingyong Su</a> there.
                </p>  
                <p style="text-align:center">
                  <a href="mailto:jdzcarr7@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jIR4PAsAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/CaraJ7">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%;object-fit: cover; " alt="profile photo" src="images/avatar.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publication</h2>
                <p>
                  I am interested in AIGC. Currently, I am focusing on Multimodal Large Language Model (MLLM) and Text-to-Image Diffusion models.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
        <img src='images/mmsearch_overview.png' width="220" height="200" style="display:block;margin:0 auto;">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2409.12959">
          <span class="papertitle"><img src="images/mmsearch.png" style="width:1em;vertical-align: middle" alt="Logo"/> MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</span>
        </a>
        <br>
        <strong>Dongzhi Jiang*</strong>,
        Renrui Zhang*,
        Ziyu Guo,
        Yanmin Wu,
        Jiayi Lei,
        Pengshuo Qiu,
        Pan Lu,
        Zehui Chen,
        Guanglu Song,
        Peng Gao,
        Yu Liu,
        Chunyuan Li,
        Hongsheng Li,
        <br>
        <em>arxiv</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2409.12959">arXiv</a>
        /
        <a href="https://mmsearch.github.io/">website</a>
        /
        <a href="https://huggingface.co/datasets/CaraJ/MMSearch">dataset</a>
        /
        <a href="https://github.com/CaraJ7/MMSearch">GitHub</a>
        <p></p>
        <p>
        We investigate the potential of current LMM to function as a multimodal AI search engine. We also introduce a multimodal AI search engine pipeline and outperforms Perplexity-pro with only open-source LMMs.
        </p>
      </td>
    </tr>

    <tr onmouseout="comat_stop()" onmouseover="comat_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='comat_image'>
            <img src='images/comat_after.png' width="260" height="170"></div>
            <img src='images/comat_before.png' width="260" height="170">
        </div>
        <script type="text/javascript">
          function comat_start() {
            document.getElementById('comat_image').style.opacity = "1";
          }

          function comat_stop() {
            document.getElementById('comat_image').style.opacity = "0";
          }
          comat_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2404.03653.pdf">
          <span class="papertitle">ðŸ’«CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</span>
        </a>
        <br>
        <strong>Dongzhi Jiang</strong>,
        Guanglu Song,
        Xiaoshi Wu,
        Renrui Zhang,
        Dazhong Shen,
        Zhuofan Zong,
        Yu Liu,
        Hongsheng Li
        <br>
        <em>arxiv</em>, 2024 
        <br>
        <a href="https://arxiv.org/abs/2404.03653">arXiv</a>
        /
        <a href="https://caraj7.github.io/comat/">website</a>
        /
        <a href="https://github.com/CaraJ7/CoMat">GitHub</a>
        <p>
        A fine-tuning strategy to address the text-to-image misalignment issue with image-to-text concept matching. The training data only includes text prompts.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/mathverse_overview.png' width="260" height="180">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2403.14624.pdf">
          <span class="papertitle"><img src="images/mathverse.png" style="width:1em;vertical-align: middle" alt="Logo"/> MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</span>
        </a>
        <br>
        Renrui Zhang*,
        <strong>Dongzhi Jiang*</strong>,
        Yichi Zhang*,
        Haokun Lin,
        Ziyu Guo,
        Pengshuo Qiu,
        Aojun Zhou,
        Pan Lu,
        Aojun Zhou,
        Kai-Wei Chang,
        Peng Gao,
        Hongsheng Li
        <br>
        <em>ECCV</em>, 2024 
        <br>
        <a href="https://arxiv.org/pdf/2403.14624.pdf">arXiv</a>
        /
        <a href="https://mathverse-cuhk.github.io/">website</a>
        /
        <a href="https://huggingface.co/datasets/AI4Math/MathVerse">dataset</a>
        /
        <a href="https://github.com/ZrrSkywalker/MathVerse">GitHub</a>
        <p></p>
        <p>
        We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.
        </p>
      </td>
    </tr>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/hop_overview.png' width="260" height="180">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2304.00967.pdf">
          <span class="papertitle">Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction</span>
        </a>
        <br>
        Zhuofan Zong*,
        <strong>Dongzhi Jiang*</strong>,
        Guanglu Song,
        Zeyue Xue,
        Jingyong Su,
        Hongsheng Li,
        Yu Liu
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2304.00967.pdf">arXiv</a>
        /
        <a href="https://github.com/Sense-X/HoP">GitHub</a>
        <p></p>
        <p>
        We design a plug-and-play approach to enhance the temporal modeling capability of BEV detectors with no additional inference cost.
        </p>
      </td>
    </tr>
           
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:13px;">
                  The source code of this website is adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
