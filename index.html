<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dongzhi Jiang</title>

    <meta name="author" content="Dongzhi Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/bat.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-family:Comic Sans MS;font-size:32px">
                  Dongzhi Jiang
                </p>
                <p class="name" style="font-size:15px">I'm a PhD student in <a href="https://mmlab.ie.cuhk.edu.hk/">Multimedia Lab</a>, CUHK, supervised by Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/"> Hongsheng Li</a>. Please email me if you have any questions or want to collaborate.</p>
                <p style="text-align:center">
                  <a href="mailto:jdzcarr7@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jIR4PAsAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/CaraJ7">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/avatar1.jpg"><img style="width:100%;max-width:100%;object-fit: cover; " alt="profile photo" src="images/avatar1.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Selected Publications</h2>
              <p>
                Currently, I am focusing on Multimodal Large Language Model (MLLM) and Text-to-Image Diffusion models.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
              <img src='images/mme_cot.jpg' width="185" height="200" style="display:block;margin:0 auto;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2502.09621">
                <span class="papertitle">MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency</span>
              </a>
              <br>
              <strong>Dongzhi Jiang*</strong>,
              Renrui Zhang*,
              Ziyu Guo,
              Yanwei Li,
              Yu Qi,
              Xinyan Chen,
              Liuhui Wang,
              Jianhan Jin,
              Claire Guo,
              Shen Yan,
              Bo Zhang,
              Chaoyou Fu,
              Peng Gao,
              Hongsheng Li
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2502.09621">arXiv</a>
              /
              <a href="https://mmecot.github.io/">website</a>
              /
              <a href="https://huggingface.co/datasets/CaraJ/MME-CoT">dataset</a>
              /
              <a href="https://github.com/CaraJ7/MME-CoT">GitHub</a>
              <p>
              A comprehensive chain-of-thought evaluation suite for large multimodal models, focusing on reasoning quality, robustness, and efficiency.
              </p>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
            <img src='images/mmsearch_overview.png' width="220" height="200" style="display:block;margin:0 auto;">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2409.12959">
              <span class="papertitle"><img src="images/mmsearch.png" style="width:1em;vertical-align: middle" alt="Logo"/> MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines</span>
            </a>
            <br>
            <strong>Dongzhi Jiang*</strong>,
            Renrui Zhang*,
            Ziyu Guo,
            Yanmin Wu,
            Jiayi Lei,
            Pengshuo Qiu,
            Pan Lu,
            Zehui Chen,
            Guanglu Song,
            Peng Gao,
            Yu Liu,
            Chunyuan Li,
            Hongsheng Li,
            <br>
            <em>ICLR</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2409.12959">arXiv</a>
            /
            <a href="https://mmsearch.github.io/">website</a>
            /
            <a href="https://huggingface.co/datasets/CaraJ/MMSearch">dataset</a>
            /
            <a href="https://github.com/CaraJ7/MMSearch">GitHub</a>
            <p></p>
            <p>
            We investigate the potential of current LMM to function as a multimodal AI search engine. We also introduce a multimodal AI search engine pipeline and outperforms Perplexity-pro with only open-source LMMs.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/mavis.jpg' width="260" height="220">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2407.08739">
              <span class="papertitle">MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine</span>
            </a>
            <br>
            Renrui Zhang,
            Xinyu Wei,
            <strong>Dongzhi Jiang</strong>,
            Ziyu Guo,
            Yichi Zhang,
            Chengzhuo Tong,
            Jiaming Liu,
            Aojun Zhou,
            Shanghang Zhang,
            Peng Gao,
            Hongsheng Li
            <br>
            <em>ICLR</em>, 2025
            <br>
            <p>An automatic data engine and specialized vision encoder for mathematical visual instruction tuning.</p>
          </td>
        </tr>



        <tr onmouseout="comat_stop()" onmouseover="comat_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='comat_image'>
                <img src='images/comat_after.png' width="260" height="170"></div>
                <img src='images/comat_before.png' width="260" height="170">
            </div>
            <script type="text/javascript">
              function comat_start() {
                document.getElementById('comat_image').style.opacity = "1";
              }

              function comat_stop() {
                document.getElementById('comat_image').style.opacity = "0";
              }
              comat_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2404.03653.pdf">
              <span class="papertitle">ðŸ’«CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</span>
            </a>
            <br>
            <strong>Dongzhi Jiang</strong>,
            Guanglu Song,
            Xiaoshi Wu,
            Renrui Zhang,
            Dazhong Shen,
            Zhuofan Zong,
            Yu Liu,
            Hongsheng Li
            <br>
            <em>NeurIPS</em>, 2024 
            <br>
            <a href="https://arxiv.org/abs/2404.03653">arXiv</a>
            /
            <a href="https://caraj7.github.io/comat/">website</a>
            /
            <a href="https://github.com/CaraJ7/CoMat">GitHub</a>
            <p>
            A fine-tuning strategy to address the text-to-image misalignment issue with image-to-text concept matching. The training data only includes text prompts.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/mova.jpg' width="260" height="120">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2404.13046">
              <span class="papertitle">MoVA: Adapting Mixture of Vision Experts to Multimodal Context</span>
            </a>
            <br>
            Zhuofan Zong,
            Bingqi Ma,
            Dazhong Shen,
            Guanglu Song,
            Hao Shao,
            <strong>Dongzhi Jiang</strong>,
            Hongsheng Li,
            Yu Liu
            <br>
            <em>NeurIPS</em>, 2024
            <br>
            <p>An adaptive approach for mixing vision experts for large multimodal model.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/easyref.png' width="260" height="110">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2412.09618">
              <span class="papertitle">EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</span>
            </a>
            <br>
            Zhuofan Zong,
            <strong>Dongzhi Jiang</strong>,
            Bingqi Ma,
            Guanglu Song,
            Hao Shao,
            Dazhong Shen,
            Yu Liu,
            Hongsheng Li
            <br>
            <em>arxiv</em>, 2024
            <br>
            <p>A novel approach for group image reference in diffusion models using large multimodal model capabilities.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/mathverse_overview.png' width="260" height="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2403.14624.pdf">
              <span class="papertitle"><img src="images/mathverse.png" style="width:1em;vertical-align: middle" alt="Logo"/> MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</span>
            </a>
            <br>
            Renrui Zhang*,
            <strong>Dongzhi Jiang*</strong>,
            Yichi Zhang*,
            Haokun Lin,
            Ziyu Guo,
            Pengshuo Qiu,
            Aojun Zhou,
            Pan Lu,
            Aojun Zhou,
            Kai-Wei Chang,
            Peng Gao,
            Hongsheng Li
            <br>
            <em>ECCV</em>, 2024 
            <br>
            <a href="https://arxiv.org/pdf/2403.14624.pdf">arXiv</a>
            /
            <a href="https://mathverse-cuhk.github.io/">website</a>
            /
            <a href="https://huggingface.co/datasets/AI4Math/MathVerse">dataset</a>
            /
            <a href="https://github.com/ZrrSkywalker/MathVerse">GitHub</a>
            <p></p>
            <p>
            We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.
            </p>
          </td>
        </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/hop_overview.png' width="260" height="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.00967.pdf">
                <span class="papertitle">Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction</span>
              </a>
              <br>
              Zhuofan Zong*,
              <strong>Dongzhi Jiang*</strong>,
              Guanglu Song,
              Zeyue Xue,
              Jingyong Su,
              Hongsheng Li,
              Yu Liu
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.00967.pdf">arXiv</a>
              /
              <a href="https://github.com/Sense-X/HoP">GitHub</a>
              <p></p>
              <p>
              We design a plug-and-play approach to enhance the temporal modeling capability of BEV detectors with no additional inference cost.
              </p>
            </td>
          </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
